runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
library(shiny)
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
install.packages("reshape")
library(reshape)
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('UVA Grades App/app-files')
runApp('app-files')
Q
runApp('UVA Grades App/app-files')
#Set up variables to convert from the grade selection (A+) to the grade format (A_plus)
grades <- c("A_tot", "A_minus", "B_plus", "B", "B_minus", "C_plus", "C", "C_minus", "Not_Passing")
names(grades) <- c("A+/A", "A-", "B+", "B", "B-", "C+", "C", "C-", "Not Passing")
grades["A+/A"]
runApp('UVA Grades App/app-files')
args(matrix)
mat <- matrix(data = c(1/3, 2/3, 3/4, 1/4), nrow = 2)
mat
mat <- matrix(data = c(1/3, 3/4, 2/3, 1/4), nrow = 2)
mat
args(exp)
4^4
2^4
mat^2
mat^1
mat %*% mat %*% mat
getwd()
setwd("/Fall 2016/STAT 5120/Homework 2")
setwd("C:/Users/Student/Documents/Fall 2016/STAT 5120/Homework 2")
setwd("C:/Users/Student/Documents/Fall 2016/STAT 5120/Homework 2")
setwd("C:/Users/Student/Documents/Fall 2016/STAT 5120/Homework 2")
i <- diag(4)
i
q <- matrix(c(0,0,0,.25,.5,0,0,0,.5,0,0,0,0,.8,.6,.5), nrows = 4)
q <- matrix(c(0,0,0,.25,.5,0,0,0,.5,0,0,0,0,.8,.6,.5), nrow = 4)
q
solve(i - q)
res <- solve(i - q)
res[1]
res[1,]
sum(res[1,])
q <- matrix(c(0,0,1,.5,0,0,0,.2,0), nrow = 3)
i < diag(3)
i <- diag(3)
q
solve(i - q)
s <- matrix(c(.5,0,0,0,.8,.6), nrow = 3)
s
res <- solve(i - q)
res
res2 <- res %*% s
res2
s
q
res
res %*% q
q
res
i
res %*% (i - q)
s <- matrix(c(.5,0,0,0,.8,0), nrow = 3)
res2 <- res %*% s
res2
install.packages(faraway)
install.packages("faraway")
mat <- matrix(c(1,1,-1,4), nrow = 2)
mat
solve(mat)
res <- solve(mat)
mat %*% res
mat <- matrix(c(-1,4,0,1,-3,4,0,1,-4), nrow = 2)
mat <- matrix(c(-1,4,0,1,-3,4,0,1,-4), nrow = 3)
mat
mat <- matrix(c(-1,4,0,1,-5,4,0,1,-4), nrow = 3)
eigen(mat)
mat <- matrix(c(1,1,1,-1,2,8,1,-6,8), nrow = 3)
res <- solve(mat)
res
mat <- matrix(c(1,1,1,-1,2,8,1,-6,8))
mat
mat <- matrix(c(1,1,1,-1,2,8,1,-6,8), nrow = 3)
mat
inv <- solve(mat)
inv
mat <- matrix(c(-3,0,1,1,-3,2,1,2,-4), nrow = 3)
mat
mat_neg = -mat
mat_neg
solve(mat_neg)
mat <- matrix(c(-2,0,1,1,-1,1,1,1-3), nrows = 3)
mat <- matrix(c(-2,0,1,1,-1,1,1,1-3), nrow = 3)
mat <- matrix(c(-2,0,1,1,-1,1,1,1,-3), nrow = 3)
mat
mat_neg <- -mat
mat_neg
mat_neg
solve(mat_neg)
mat <- matrix(c(0, .5, 0,0,0,0,0,.5,0,.,5,0,0,0,0,0,.5,0,0,0,0,0,0,0,0,0,.5,0,0,0,0,0,.5,0,0,0,0,0,0,0,.5,0,.5,0,0,0,0,0,.5,0), nrow = 7)
mat <- matrix(c(0, 1/2, 0,0,0,0,0,1/2,0,1/2,0,0,0,0,0,1/2,0,0,0,0,0,0,0,0,0,1/2,0,0,0,0,0,1/2,0,0,0,0,0,0,0,1/2,0,1/2,0,0,0,0,0,1/2,0), nrow = 7)
mat
s <- matrix(c(1/2, 0,0,0,0,0,0,0,0,1/2,1/2,0,0,0,0,0,0,0,0,0,1/2,0,0,0,0,0,0,0), nrow = 7)
s
i <- daig(7)
i <- diag(7)
i
solve(i - mat)
res <- solve(i - mat)
res %*% s
s %*% res
res
res %*% s
mat <- matrix(c(0, 1/2, 0,0,0,0,0,1/2,0,1/2,0,0,0,0,0,1/2,0,0,0,0,0,0,0,0,0,1/2,0,0,0,0,0,1/2,0,1/2,0,0,0,0,0,1/2,0,1/2,0,0,0,0,0,1/2,0), nrow = 7)
res <- solve(i - mat)
res %*% s
#Question 1
masim <- function(sigs, sigsq, T){
#Find the number of previous terms we rely on
q<-length(sigs)
#Generate our white noise
noise<-rnorm(T+q, sd=sqrt(sigsq))
#Get our time series array set up
x<-c(noise[1:q],rep(0, T))
#Look to generate our series
for(i in (q+1):(T+q)){
x[i]<-noise[i]+(noise[i-(1:q)] %*% sigs)
}
x<-x[(q+1):(T+q)]
return(x)
}
x <- masim(c(0.5,2), 1, 10000)
acf(x)
y <- masim(c(0.5,2), 1, 200)
acf(y)
y <- masim(c(0.5,2), 1, 200)
acf(y)
x <- c(4,5,6,7,8,9)
y <- c(29.5, 31,32,33,36,39)
mod <- lm(y ~ x)
mod
x <- c(0,1,2,3,4,5)
y <- c(6,9,9.8,12,12.7,14.3)
mod <- lm(y ~ x)
mod
power.aw<-function(sigsq,omega)
{
aw<-sigsq*(4/9*cos(2*pi*omega)^2-4/3*cos(2*pi*omega)+4/9)
}
frequency<-seq(0,0.5,by=0.01)
power.aw<-function(sigsq,omega)
{
aw<-sigsq*(4/9*cos(2*pi*omega)^2-8/9*cos(2*pi*omega)+4/9)
}
frequency<-seq(0,0.5,by=0.01)
aw1<-power.aw(1,frequency)
plot(frequency,aw1, type="l", main="Power Transfer Function")
1 + 2
install.packages("tidyverse")
tidyverse::tidyverse_update(recursive = TRUE)
tidyverse::tidyverse_update()
library("acepack", lib.loc="~/R/win-library/3.3")
library("astsa", lib.loc="~/R/win-library/3.3")
library("assertthat", lib.loc="~/R/win-library/3.3")
detach("package:astsa", unload=TRUE)
detach("package:assertthat", unload=TRUE)
detach("package:acepack", unload=TRUE)
install.packages(c("assertthat", "BH", "chron", "colorspace", "data.table", "DBI", "digest", "evaluate", "formatR", "ggthemes", "Hmisc", "htmlTable", "htmltools", "knitr", "lawstat", "leaps", "lme4", "markdown", "mvtnorm", "R6", "Rcpp", "RcppEigen", "reshape", "reshape2", "shiny", "stringi", "survival", "VGAM", "viridis", "yaml"))
install.packages(c("assertthat", "BH", "chron", "colorspace", "data.table", "DBI", "digest", "evaluate", "formatR", "ggthemes", "Hmisc", "htmlTable", "htmltools", "knitr", "lawstat", "leaps", "lme4", "markdown", "mvtnorm", "R6", "Rcpp", "RcppEigen", "reshape", "reshape2", "shiny", "stringi", "survival", "VGAM", "viridis", "yaml"))
library("tidyverse")
library(tidyverse)
install.packages(tidyverse)
install.packages("tidyverse")
library(tidyverse)
tidyverse()
help("tidyverse")
library(ggplot2)
version()
R.version
R.version
R.version
R.version()
R.version
library(ggplot2)
install.packages(ggplot2)
install.packages("ggplot2")
library(tidyvers)
library(tidyvers)
library(tidyverse)
install.packages("dplyr")
install.packages("dplyr")
help("install.packages")
help("install.packages")
install.packages("dplyr")
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("dplyr")
install.packages("tidyverse")
library(dplyr)
library(ggplot2)
install.packages("dplyr")
sessionInfo()
library(tidyverse)
tidyverse::tidyverse_update()
tidyverse::tidyverse_update()
?assignOps
updateR()
library(Rcpp)
getwd()
sourceCpp(file = "test.cpp")
sourceCpp(file = "test.cpp")
fib(3)
library(projmanr)
taskdata1
critical_path(taskdata1)
setwd("~/Fall 2017/Data Mining/Kaggle/self-driving-cars")
library(tidyverse)
library(tm)
library(XML)
train <- read_csv("train.csv") #Read in the comma separated value data file for training the model
test <- read_csv("test.csv") #Read in the csv data file for testing the model
sample <- read_csv("sample.csv") #Read in the csv data file for sample submission (for reference)
tweets = VCorpus(DataframeSource(train[,2]))
tweets.tfidf = DocumentTermMatrix(tweets, control = list(weighting = weightTfIdf))
tweets.tfidf  # non-/sparse entries indicates how many of the DTM cells are non-zero and zero, respectively.
tweets.clean = tm_map(tweets, stripWhitespace)                          # remove extra whitespace
tweets.clean = tm_map(tweets.clean, removeNumbers)                      # remove numbers
tweets.clean = tm_map(tweets.clean, removePunctuation)                  # remove punctuation
tweets.clean = tm_map(tweets.clean, content_transformer(tolower))       # ignore case
tweets.clean = tm_map(tweets.clean, removeWords, stopwords("english"))  # remove stop words
tweets.clean = tm_map(tweets.clean, stemDocument)                       # stem all words
tweets[[1]]$content
tweets.clean[[1]]$content  # do we care about misspellings resulting from stemming?
tweets.clean.tfidf = DocumentTermMatrix(tweets.clean, control = list(weighting = weightTfIdf))
tweets.clean.tfidf[1:5,1:5]
as.matrix(tweets.clean.tfidf[1:5,1:5])
tfidf.99 = removeSparseTerms(tweets.clean.tfidf, 0.99)  #Remove terms that are absent from at least 99% of documents (keep most terms)
tfidf.99
as.matrix(tfidf.99[1:5,1:5])
dtm.tfidf.99 = as.matrix(tfidf.99)
df.99.scored <- data.frame(dtm.tfidf.99)
df.99.scored["SCORE"] <- train[,1]
colnames.99 <- as.list(colnames(df.99.scored))
colnames.99
lm.df.99 <- lm(SCORE~.,
data=df.99.scored)
summary(lm.df.99)
lm.df.99.2 <- lm(SCORE~cant+dont+excit+googl+insur+less+need+pedal+safer+save+saw+
soon+thing+wait+want+warn+wrong, #Select anything with significance
data=df.99.scored)
summary(lm.df.99.2)
lm.df.99.3 <- lm(SCORE~cant+dont+excit+googl+insur+less+need+safer+
soon+thing+wait+want+warn+wrong, #Select anything with significance
data=df.99.scored)
summary(lm.df.99.3)
colnames.99
step(lm.df.99)
step(lm.df.99, direction = "both")
lm.optimal <- lm(formula = SCORE ~ accid + cant + car + come + cool + dont +
excit + fbi + googl + hit + insur + just + less + need +
pedal + safer + save + saw + soon + taxi + thing + think +
truck + wait + want + warn + wheel + will + wrong + yes,
data = df.99.scored)
summary(lm.optimal)
summary(lm.df.99.3)
test = VCorpus(DataframeSource(test[,2]))
test.tfidf = DocumentTermMatrix(test, control = list(weighting = weightTfIdf))
test.tfidf  # non-/sparse entries indicates how many of the DTM cells are non-zero and zero, respectively.
test.clean = tm_map(test, stripWhitespace)                          # remove extra whitespace
test.clean = tm_map(test.clean, removeNumbers)                      # remove numbers
test.clean = tm_map(test.clean, removePunctuation)                  # remove punctuation
test.clean = tm_map(test.clean, content_transformer(tolower))       # ignore case
test.clean = tm_map(test.clean, removeWords, stopwords("english"))  # remove stop words
test.clean = tm_map(test.clean, stemDocument)                       # stem all words
test.clean.tfidf = DocumentTermMatrix(test.clean, control = list(weighting = weightTfIdf))
test.clean.tfidf = as.matrix(test.clean.tfidf)
df.test.preds <- data.frame(test.clean.tfidf)
mypreds <- data.frame(predict(lm.optimal, newdata = df.test.preds))
mypreds <- data.frame(predict(lm.optimal, newdata = df.test.preds))
test.clean.tfidf = DocumentTermMatrix(test.clean, control = list(weighting = weightTfIdf,
dictionary = colnames(df.99.scored)))
test.clean.tfidf = as.matrix(test.clean.tfidf)
df.test.preds <- data.frame(test.clean.tfidf)
mypreds <- data.frame(predict(lm.optimal, newdata = df.test.preds))
sentiment <- round(mypreds[,1],digits=0)
lm.preds <- as.data.frame(sentiment)
lm.preds
mypreds
test.clean
data.frame(test.clean)
lm.preds[lm.preds > 5,] = 5
lm.preds[lm.preds < 1,] = 1
lm.preds
test.clean$content
lm.preds
temp <- lm.preds
test = VCorpus(DataframeSource(test[,2]))
test <- read_csv("test.csv") #Read in the csv data file for testing the model
test = VCorpus(DataframeSource(test[,2]))
test.tfidf = DocumentTermMatrix(test, control = list(weighting = weightTfIdf))
test.tfidf  # non-/sparse entries indicates how many of the DTM cells are non-zero and zero, respectively.
test.clean = tm_map(test, stripWhitespace)                          # remove extra whitespace
test.clean = tm_map(test.clean, removeNumbers)                      # remove numbers
test.clean = tm_map(test.clean, removePunctuation)                  # remove punctuation
test.clean = tm_map(test.clean, content_transformer(tolower))       # ignore case
test.clean = tm_map(test.clean, removeWords, stopwords("english"))  # remove stop words
test.clean = tm_map(test.clean, stemDocument)                       # stem all words
test.clean.tfidf = DocumentTermMatrix(test.clean, control = list(weighting = weightTfIdf))
test.clean.tfidf = as.matrix(test.clean.tfidf)
df.test.preds <- data.frame(test.clean.tfidf)
View(df.test.preds)
test.clean.tfidf = as.matrix(test.clean.tfidf)
df.test.preds <- data.frame(test.clean.tfidf)
mypreds <- data.frame(predict(lm.df.99.3, newdata = df.test.preds))
sentiment <- round(mypreds[,1],digits=0)
lm.preds <- as.data.frame(sentiment)
lm.preds[lm.preds > 5,] = 5
lm.preds[lm.preds < 1,] = 1
lm.preds
lm.preds[, 2] == temp[, 2]
lm.preds$sentiment == temp[, 2]
lm.preds$sentiment == temp$sentiment
sum(lm.preds$sentiment == temp$sentiment)
test.clean.tfidf = DocumentTermMatrix(test.clean, control = list(weighting = weightTfIdf))
test.clean.tfidf = DocumentTermMatrix(test.clean, control = list(weighting = weightTfIdf),
dictionary = colnames(df.99.scored)))
test.clean.tfidf = DocumentTermMatrix(test.clean, control = list(weighting = weightTfIdf,
dictionary = colnames(df.99.scored)))
test.clean.tfidf = as.matrix(test.clean.tfidf)
df.test.preds <- data.frame(test.clean.tfidf)
mypreds <- data.frame(predict(lm.optimal, newdata = df.test.preds))
sentiment <- round(mypreds[,1],digits=0)
lm.preds <- as.data.frame(sentiment)
lm.preds[lm.preds > 5,] = 5
lm.preds[lm.preds < 1,] = 1
test <- read_csv("test.csv") #Read in the csv data file for testing the model
lm.preds["id"] = test[,1]
lm.preds <- lm.preds[c(2,1)] #Switch columns
write.table(lm.preds, file = "lm_car_tweets_bhg.csv", row.names=F, sep=",") #Write out to a csv
library(tm)
library(readr)
source("knn.R")
library(MASS) # Used for lda
source("preprocess.R")
library(class)
?knn
train <- read_csv("train.csv")
preds <- clean_data(train, 0.99, F, T, F)
k <- seq(1, 2 * as.integer(sqrt(nrow(preds))), by = 2)
reses <- vector(mode = "numeric", length = length(k))
for(i in 1:length(k)){
p <- knn.cv(preds, as.factor(train$sentiment), k = k[i])
reses[i] <- sum(p == train$sentiment)/length(train$sentiment)
}
reses
sparc <- c(0.95, 0.96, 0.975, 0.985, 0.99)
reses <- vector(mode = "numeric", length = length(sparc))
for(i in 1:length(sparc)){
preds <- clean_data(train, sparc[i], F, T, F)
p <- knn.cv(preds, as.factor(train$sentiment), k = 19)
reses[i] <- sum(p == train$sentiment)/length(train$sentiment)
}
reses
preds <- clean_data(train, 0.975, F, T, F)
p <- knn.cv(preds, as.factor(train$sentiment), k = 19)
sum(p == train$sentiment)/length(train$sentiment)
preds <- clean_data(train, 0.975, T, T, F)
p <- knn.cv(preds, as.factor(train$sentiment), k = 19)
sum(p == train$sentiment)/length(train$sentiment)
preds <- clean_data(train, 0.975, F, F, F)
p <- knn.cv(preds, as.factor(train$sentiment), k = 19)
sum(p == train$sentiment)/length(train$sentiment)
preds <- clean_data(train, 0.975, F, T, F)
p <- knn.cv(preds, as.factor(train$sentiment), k = 19)
sum(p == train$sentiment)/length(train$sentiment)
preds <- clean_data(train, 0.975, F, T, F)
p <- knn.cv(preds, as.factor(train$sentiment), k = 19)
sum(p == train$sentiment)/length(train$sentiment)
preds <- clean_data(train, 0.975, F, T, T)
p <- knn.cv(preds, as.factor(train$sentiment), k = 19)
sum(p == train$sentiment)/length(train$sentiment)
train.data <- clean_data(train, 0.975, F, T, T)
train.data
is.na(NULL)
NULL == NULL
is.null(NULL)
test.data <- clean_data(test, 0.975, F, T, T, colnames(train.data))
source("preprocess.R")
train.data <- clean_data(train, 0.975, F, T, T)
test.data <- clean_data(test, 0.975, F, T, T, colnames(train.data))
View(test.data)
View(train.data)
colnames(train.data)
colnames(train.data)[colnames(train.data) == "num_hash"]
colnames(train.data)[colnames(train.data) %in% "num_hash"]
colnames(train.data)[colnames(train.data) %in% c("num_hash", "num_at", "num_exlaim", "num_question"]
colnames(train.data)[colnames(train.data) %in% c("num_hash", "num_at", "num_exlaim", "num_question")]
colnames(train.data)[colnames(train.data) %in% c("num_hash", "num_at", "num_exlaim", "num_question")] <- NULL
colnames(train.data)[colnames(train.data) !%in% c("num_hash", "num_at", "num_exlaim", "num_question")]
colnames(train.data)[colnames(train.data) %in% c("num_hash", "num_at", "num_exlaim", "num_question")]
colnames(train.data)[colnames(train.data) %in% c("num_hash", "num_at", "num_exlaim", "num_question")] = NULL
train.data <- clean_data(train, 0.975, F, T, F)
test.data <- clean_data(test, 0.975, F, T, T, colnames(train.data))
View(train.data)
train.data <- clean_data(train, 0.975, F, T, F)
test.data <- clean_data(test, 0.975, F, T, F, colnames(train.data))
source("preprocess.R")
test.data <- clean_data(test, 0.975, F, T, F, colnames(train.data))
View(test.data)
View(train.data)
train
test
paste(colnames(train.data))
?paste
paste(colnames(train.data), " ")
paste(colnames(train.data), collapse = " ")
test <- rbind(test, c(1000, paste(colnames(train.data), collapse = " ")))
View(test)
test.data <- clean_data(test, 0.975, F, T, F, colnames(train.data))
source("preprocess.R")
test.data <- clean_data(test, 0.975, F, T, F, colnames(train.data))
View(test.data)
test.data[-nrow(test.data), ]
source("preprocess.R")
train.data <- clean_data(train, 0.975, F, T, F)
test <- rbind(test, c(1000, paste(colnames(train.data), collapse = " ")))
test.data <- clean_data(test, 0.975, F, T, F, colnames(train.data))
test <- read_csv("test.csv")
test.data <- rbind(test, c(1000, paste(colnames(train.data), collapse = " ")))
test.data <- clean_data(test.data, 0.975, F, T, F, colnames(train.data))
predicts <- knn(train = train.data, test = test.data, cl = train$sentiment)
predicts
train.data <- clean_data(train, 0.975, F, T, T)
test.data <- rbind(test, c(1000, paste(colnames(train.data), collapse = " ")))
test.data <- clean_data(test.data, 0.975, F, T, F, colnames(train.data))
train.data <- clean_data(train, 0.975, F, T, T)
View(train.data)
test.data <- rbind(test, c(1000, paste(colnames(train.data), collapse = " ")))
View(test.data)
test.data[980, text]
test.data
test.data$text
source("preprocess.R")
train.data <- clean_data(train, 0.975, F, T, T)
test.data <- rbind(test, c(1000, paste(colnames(train.data), collapse = " ")))
test.data <- clean_data(test.data, 0.975, F, T, F, colnames(train.data))
View(test.data)
train.data <- clean_data(train, 0.975, F, T, F)
test.data <- rbind(test, c(1000, paste(colnames(train.data), collapse = " ")))
test.data <- clean_data(test.data, 0.975, F, T, F, colnames(train.data))
train.data <- cbind(train.data, feature_extract(train.data))
train.data <- cbind(train.data, feature_extract(train))
test.data <- cbind(test.data, feature_extract(test))
predicts <- knn(train = train.data, test = test.data, cl = train$sentiment)
View(train.data)
source("preprocess.R")
train.data <- clean_data(train, 0.975, F, T, F)
test.data <- rbind(test, c(1000, paste(colnames(train.data), collapse = " ")))
test.data <- clean_data(test.data, 0.975, F, T, F, colnames(train.data))
train.data <- cbind(train.data, feature_extract(train))
test.data <- cbind(test.data, feature_extract(test))
predicts <- knn(train = train.data, test = test.data, cl = train$sentiment)
predicts
predicts <- knn(train = train.data, test = test.data, cl = train$sentiment, k = 19)
predicts
predicts <- knn(train = train.data, test = test.data, cl = train$sentiment, k = 17)
predicts
sum(train$sentiment == 3)/length(train$sentiment)
for(i in 1:19){
predicts <- knn(train = train.data, test = test.data, cl = train$sentiment, k = i)
sum(predicts == 3)/length(predicts)
}
print(sum(predicts == 3)/length(predicts))
reses <- vector(mode = "numeric", length = 19)
for(i in 1:19){
predicts <- knn(train = train.data, test = test.data, cl = train$sentiment, k = i)
reses[i] <- sum(predicts == 3)/length(predicts)
}
reses
sum(train$sentiment == 3)/length(train$sentiment)
results <- cbind("id" = test$id, "sentiment" = predicts)
View(results)
write_csv(results, "predictions_knn_ben.csv")
write_csv(data.frame(results), "predictions_knn_ben.csv")
predicts <- knn(train = train.data, test = test.data, cl = train$sentiment, k = 17)
results <- cbind("id" = test$id, "sentiment" = predicts)
write_csv(data.frame(results), "predictions_knn_ben.csv")
